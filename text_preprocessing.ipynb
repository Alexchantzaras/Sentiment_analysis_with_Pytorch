{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Preprocessing\n",
    "\n",
    "Load covid tweets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "tweets_covid = pd.read_csv(\"covid19_tweets.csv\")\n",
    "tweets_covid.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation\n",
    "To clean the data we need to remove the **links**, **punctuation**, **numbers**, **emojis**, and **stop words**. We will utilize nltk's english stopword and wordnet databases to filter out unwanted words and then normalize the rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download nltk's databases\n",
    "nltk.download('all')\n",
    "\n",
    "def remove_stopwords(txt):\n",
    "    \"\"\"Remove stopwords from the input text\n",
    "\n",
    "    Args:\n",
    "        txt (str): the input text to filter\n",
    "\n",
    "    Returns:\n",
    "        str: the filtered text, with all stopwords removed\n",
    "    \"\"\"\n",
    "    words = txt.lower().split()\n",
    "    non_stopwords = [word for word in words if word not in stop_words]\n",
    "    non_stopwords = ' '.join(non_stopwords)\n",
    "    return non_stopwords\n",
    "\n",
    "# Create a hash-set containing all stopwords\n",
    "# which automatically guarantees word uniqueness\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Filter out links\n",
    "tweets_covid['clean_text'] = tweets_covid['text'].apply(lambda s: ' '.join(re.sub(\"(w+://S+)\", \" \", s).split()))\n",
    "# Filter out punctuation\n",
    "tweets_covid[\"clean_text\"] = tweets_covid[\"clean_text\"].apply(lambda s: ' '.join(re.sub(\"[.,!?:;-='...@#_]\", \" \", s).split()))\n",
    "# Filter out numerical values\n",
    "tweets_covid[\"clean_text\"] = tweets_covid[\"clean_text\"].apply(lambda s: ' '.join(re.sub(\"\\d\", \"\", s).split()))\n",
    "# Filter out emojis, first turn into ascii and then back to utf\n",
    "tweets_covid[\"clean_text\"] = tweets_covid[\"clean_text\"].apply(lambda s: s.encode('ascii', 'ignore').decode('ascii'))\n",
    "# Filter out stopwords\n",
    "tweets_covid[\"clean_text\"] = tweets_covid[\"clean_text\"].apply(lambda s: remove_stopwords(s))\n",
    "# Print sample output\n",
    "tweets_covid[['text', 'clean_text']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize the clean text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split each line on every whitespace\n",
    "tweets_covid['clean_text'] = tweets_covid['clean_text'].apply(lambda s: s.split())\n",
    "tweets_covid[['text', 'clean_text']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Normalization\n",
    "\n",
    "At this stage we want to convert words to their base form. This will produce the root form of all words, which will help our models later on during training and inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatiser = WordNetLemmatizer()\n",
    "# Apply text normalization\n",
    "tweets_covid['clean_text'] = tweets_covid['clean_text'].apply(lambda tokens: [lemmatiser.lemmatize(token, pos='v') for token in tokens])\n",
    "tweets_covid[['text', 'clean_text']]"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7f46a541a5fcb097b81b0a1ed969985dcdc63c81f34ae8abef835ad67b1a564a"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
